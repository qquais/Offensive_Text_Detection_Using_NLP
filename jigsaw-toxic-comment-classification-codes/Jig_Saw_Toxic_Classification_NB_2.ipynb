{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project: Detecting Offensive Text Using NLP Techniques\n",
    "# Subject: CS 59000-05 Natural Language Processing\n",
    "# Author: \n",
    "# Qurratul Ain Quais : quaiqa@pfw.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "\n",
      "Example Comments:\n",
      "Example 1:\n",
      "Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "\n",
      "Example 2:\n",
      "\"\n",
      "And ... I really don't think you understand.  I came here and my idea was bad right away.  What kind of community goes \"\"you have bad ideas\"\" go away, instead of helping rewrite them.   \"\n",
      "\n",
      "\n",
      "Analyzing comment lengths...\n",
      "Mean length: 394.07\n",
      "Standard deviation: 590.72\n",
      "Max length: 5000\n",
      "\n",
      "Analyzing label distribution...\n",
      "\n",
      "Label Distribution:\n",
      "toxic: 9.58%\n",
      "severe_toxic: 1.00%\n",
      "obscene: 5.29%\n",
      "threat: 0.30%\n",
      "insult: 4.94%\n",
      "identity_hate: 0.88%\n",
      "\n",
      "Creating general toxic/non-toxic word clouds...\n",
      "\n",
      "Analyzing label correlations...\n",
      "\n",
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/nlpprj/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (159571, 425935)\n",
      "Testing features shape: (153164, 425935)\n",
      "\n",
      "Training models and making predictions...\n",
      "Training model for toxic...\n",
      "Training model for severe_toxic...\n",
      "Training model for obscene...\n",
      "Training model for threat...\n",
      "Training model for insult...\n",
      "Training model for identity_hate...\n",
      "\n",
      "Calculating AUC scores...\n",
      "toxic: 0.9664\n",
      "severe_toxic: 0.9817\n",
      "obscene: 0.9754\n",
      "threat: 0.9915\n",
      "insult: 0.9699\n",
      "identity_hate: 0.9757\n",
      "\n",
      "Mean ROC AUC Score: 0.9768\n",
      "\n",
      "Calculating comprehensive metrics...\n",
      "\n",
      "Metrics for toxic:\n",
      "Precision: 0.6148\n",
      "Recall: 0.7823\n",
      "F1-score: 0.6885\n",
      "Accuracy: 0.9326\n",
      "AUC: 0.9664\n",
      "\n",
      "Metrics for severe_toxic:\n",
      "Precision: 0.3491\n",
      "Recall: 0.3815\n",
      "F1-score: 0.3646\n",
      "Accuracy: 0.9924\n",
      "AUC: 0.9817\n",
      "\n",
      "Metrics for obscene:\n",
      "Precision: 0.7199\n",
      "Recall: 0.6852\n",
      "F1-score: 0.7021\n",
      "Accuracy: 0.9665\n",
      "AUC: 0.9754\n",
      "\n",
      "Metrics for threat:\n",
      "Precision: 0.6015\n",
      "Recall: 0.3791\n",
      "F1-score: 0.4651\n",
      "Accuracy: 0.9971\n",
      "AUC: 0.9915\n",
      "\n",
      "Metrics for insult:\n",
      "Precision: 0.7366\n",
      "Recall: 0.5515\n",
      "F1-score: 0.6307\n",
      "Accuracy: 0.9654\n",
      "AUC: 0.9699\n",
      "\n",
      "Metrics for identity_hate:\n",
      "Precision: 0.6509\n",
      "Recall: 0.3876\n",
      "F1-score: 0.4859\n",
      "Accuracy: 0.9909\n",
      "AUC: 0.9757\n",
      "\n",
      "Mean Metrics Across All Labels:\n",
      "Mean precision: 0.6121\n",
      "Mean recall: 0.5279\n",
      "Mean f1_score: 0.5562\n",
      "Mean accuracy: 0.9741\n",
      "Mean auc: 0.9768\n",
      "\n",
      "Analysis complete! Check the generated files for visualizations and detailed results.\n"
     ]
    }
   ],
   "source": [
    "# Define label columns\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load and prepare the training and testing data\"\"\"\n",
    "    print(\"Loading dataset...\")\n",
    "    train = pd.read_csv('train.csv')\n",
    "    test = pd.read_csv('test.csv')\n",
    "    test_labels = pd.read_csv('test_labels.csv')\n",
    "    \n",
    "    # Create 'none' label for comments with no toxicity labels\n",
    "    train['none'] = 1 - train[label_cols].max(axis=1)\n",
    "    \n",
    "    # Handle missing values\n",
    "    train['comment_text'] = train['comment_text'].fillna(\"unknown\")\n",
    "    test['comment_text'] = test['comment_text'].fillna(\"unknown\")\n",
    "    \n",
    "    return train, test, test_labels\n",
    "\n",
    "def analyze_text_examples(train_df):\n",
    "    \"\"\"Print example comments\"\"\"\n",
    "    print(\"\\nExample Comments:\")\n",
    "    print('Example 1:\\n{}\\n'.format(train_df['comment_text'][0]))\n",
    "    print('Example 2:\\n{}\\n'.format(train_df['comment_text'][159570]))\n",
    "\n",
    "def analyze_comment_lengths(df, title='Comment Length Distribution'):\n",
    "    \"\"\"Analyze and visualize comment lengths\"\"\"\n",
    "    print(\"\\nAnalyzing comment lengths...\")\n",
    "    lens = df.comment_text.str.len()\n",
    "    mean, std, max_len = lens.mean(), lens.std(), lens.max()\n",
    "    print(f\"Mean length: {mean:.2f}\")\n",
    "    print(f\"Standard deviation: {std:.2f}\")\n",
    "    print(f\"Max length: {max_len}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    lens.hist(bins=50)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Comment Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.savefig('comment_lengths.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return {'mean': mean, 'std': std, 'max': max_len}\n",
    "\n",
    "def analyze_label_distribution(train_df):\n",
    "    \"\"\"Analyze the distribution of labels\"\"\"\n",
    "    print(\"\\nAnalyzing label distribution...\")\n",
    "    # Calculate label frequencies\n",
    "    label_counts = train_df[label_cols].sum()\n",
    "    total_comments = len(train_df)\n",
    "    \n",
    "    # Create distribution plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    label_counts.plot(kind='bar')\n",
    "    plt.title('Distribution of Toxic Labels')\n",
    "    plt.xlabel('Label Type')\n",
    "    plt.ylabel('Number of Comments')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('label_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Calculate and print percentages\n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    for label in label_cols:\n",
    "        percentage = (train_df[label].sum() / total_comments) * 100\n",
    "        print(f\"{label}: {percentage:.2f}%\")\n",
    "    \n",
    "    return label_counts.to_dict()\n",
    "\n",
    "def tokenize(s): \n",
    "    \"\"\"Tokenize text\"\"\"\n",
    "    re_tok = re.compile(f'([{string.punctuation}\"\"¨«»®´·º½¾¿¡§£₤''])')\n",
    "    return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "def extract_features(train_df, test_df):\n",
    "    \"\"\"Extract TF-IDF features\"\"\"\n",
    "    print(\"\\nExtracting features...\")\n",
    "    vec = TfidfVectorizer(\n",
    "        ngram_range=(1,2),\n",
    "        tokenizer=tokenize,\n",
    "        min_df=3,\n",
    "        max_df=0.9,\n",
    "        strip_accents='unicode',\n",
    "        use_idf=True,\n",
    "        smooth_idf=True,\n",
    "        sublinear_tf=True\n",
    "    )\n",
    "    \n",
    "    train_features = vec.fit_transform(train_df['comment_text'])\n",
    "    test_features = vec.transform(test_df['comment_text'])\n",
    "    \n",
    "    print(f\"Training features shape: {train_features.shape}\")\n",
    "    print(f\"Testing features shape: {test_features.shape}\")\n",
    "    \n",
    "    return train_features, test_features, vec\n",
    "\n",
    "def create_general_wordclouds(df, label_cols):\n",
    "    \"\"\"Create word clouds for overall toxic and non-toxic comments\"\"\"\n",
    "    print(\"\\nCreating general toxic/non-toxic word clouds...\")\n",
    "    \n",
    "    # A comment is toxic if any toxicity label is 1\n",
    "    toxic_mask = df[label_cols].max(axis=1) == 1\n",
    "    \n",
    "    # Create toxic wordcloud\n",
    "    toxic_text = ' '.join(df[toxic_mask]['comment_text'])\n",
    "    toxic_cloud = WordCloud(width=800, height=400, \n",
    "                          background_color='white',\n",
    "                          max_words=200).generate(toxic_text)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(toxic_cloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Overall Toxic Comments WordCloud')\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig('overall_toxic_wordcloud.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create non-toxic wordcloud\n",
    "    non_toxic_text = ' '.join(df[~toxic_mask]['comment_text'])\n",
    "    non_toxic_cloud = WordCloud(width=800, height=400, \n",
    "                               background_color='white',\n",
    "                               max_words=200).generate(non_toxic_text)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(non_toxic_cloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Overall Non-Toxic Comments WordCloud')\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig('overall_non_toxic_wordcloud.png')\n",
    "    plt.close()\n",
    "\n",
    "def create_wordcloud_by_label(df, label, title):\n",
    "    \"\"\"Create wordcloud for specific label\"\"\"\n",
    "    text = ' '.join(df[df[label] == 1]['comment_text'])\n",
    "    wordcloud = WordCloud(width=800, height=400, \n",
    "                         background_color='white',\n",
    "                         max_words=200).generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(f'wordcloud_{label}.png')\n",
    "    plt.close()\n",
    "\n",
    "def analyze_correlations(df):\n",
    "    \"\"\"Analyze correlations between labels\"\"\"\n",
    "    print(\"\\nAnalyzing label correlations...\")\n",
    "    corr_matrix = df[label_cols].corr()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Label Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return corr_matrix\n",
    "\n",
    "def calculate_metrics(predictions_df, test_labels):\n",
    "    \"\"\"Calculate comprehensive metrics including Precision, Recall, F1-score, and Accuracy\"\"\"\n",
    "    metrics_results = {}\n",
    "    \n",
    "    # Lists to store metrics for mean calculation\n",
    "    all_metrics = {\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1_score': [],\n",
    "        'accuracy': [],\n",
    "        'auc': []\n",
    "    }\n",
    "    \n",
    "    for col in label_cols:\n",
    "        # Convert predictions to binary using 0.5 threshold\n",
    "        y_pred_binary = (predictions_df[col] >= 0.5).astype(int)\n",
    "        y_true = test_labels[col].values\n",
    "        \n",
    "        # Filter out -1 labels if they exist\n",
    "        valid_indices = y_true != -1\n",
    "        y_pred_binary = y_pred_binary[valid_indices]\n",
    "        y_true = y_true[valid_indices]\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred_binary).ravel()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        auc = roc_auc_score(y_true, predictions_df[col][valid_indices]) if len(np.unique(y_true)) > 1 else 0\n",
    "        \n",
    "        # Store metrics for mean calculation\n",
    "        all_metrics['precision'].append(precision)\n",
    "        all_metrics['recall'].append(recall)\n",
    "        all_metrics['f1_score'].append(f1)\n",
    "        all_metrics['accuracy'].append(accuracy)\n",
    "        all_metrics['auc'].append(auc)\n",
    "        \n",
    "        metrics_results[col] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'accuracy': accuracy,\n",
    "            'auc': auc,\n",
    "            'confusion_matrix': {\n",
    "                'tn': tn,\n",
    "                'fp': fp,\n",
    "                'fn': fn,\n",
    "                'tp': tp\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Print metrics for current label\n",
    "        print(f\"\\nMetrics for {col}:\")\n",
    "        print(f\"Precision: {metrics_results[col]['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics_results[col]['recall']:.4f}\")\n",
    "        print(f\"F1-score: {metrics_results[col]['f1_score']:.4f}\")\n",
    "        print(f\"Accuracy: {metrics_results[col]['accuracy']:.4f}\")\n",
    "        print(f\"AUC: {metrics_results[col]['auc']:.4f}\")\n",
    "        \n",
    "        # Create confusion matrix visualization\n",
    "        cm = np.array([[tn, fp], [fn, tp]])\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix for {col}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.savefig(f'confusion_matrix_{col}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Calculate mean metrics\n",
    "    mean_metrics = {metric: np.mean(values) for metric, values in all_metrics.items()}\n",
    "    \n",
    "    # Print mean metrics\n",
    "    print(\"\\nMean Metrics Across All Labels:\")\n",
    "    for metric, value in mean_metrics.items():\n",
    "        print(f\"Mean {metric}: {value:.4f}\")\n",
    "    \n",
    "    # Add mean metrics to results\n",
    "    metrics_results['mean_metrics'] = mean_metrics\n",
    "    \n",
    "    return metrics_results\n",
    "\n",
    "def visualize_all_metrics(metrics_results):\n",
    "    \"\"\"Create a comprehensive visualization of all metrics\"\"\"\n",
    "    metrics_to_plot = ['precision', 'recall', 'f1_score', 'accuracy', 'auc']\n",
    "    n_metrics = len(metrics_to_plot)\n",
    "    \n",
    "    # Create main metrics comparison plot\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    width = 0.15\n",
    "    x = np.arange(len(label_cols))\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        values = [metrics_results[col][metric] for col in label_cols]\n",
    "        bars = plt.bar(x + (i - n_metrics/2 + 0.5) * width, values, width, \n",
    "                      label=metric.replace('_', ' ').title())\n",
    "        \n",
    "        # Add mean line for each metric\n",
    "        mean_value = metrics_results['mean_metrics'][metric]\n",
    "        plt.axhline(y=mean_value, color=bars.patches[0].get_facecolor(), \n",
    "                   linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.xlabel('Labels')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('All Metrics Comparison Across Labels (with Means)')\n",
    "    plt.xticks(x, label_cols, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('all_metrics_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create mean metrics summary plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    mean_values = [metrics_results['mean_metrics'][metric] for metric in metrics_to_plot]\n",
    "    \n",
    "    bars = plt.bar(metrics_to_plot, mean_values)\n",
    "    plt.title('Mean Metrics Across All Labels')\n",
    "    plt.xlabel('Metric')\n",
    "    plt.ylabel('Mean Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mean_metrics_summary.png')\n",
    "    plt.close()\n",
    "\n",
    "def save_metrics_summary(metrics_results):\n",
    "    \"\"\"Save a detailed summary of all metrics to a file\"\"\"\n",
    "    with open('metrics_summary.txt', 'w') as f:\n",
    "        f.write(\"METRICS SUMMARY REPORT\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        # Write individual label metrics\n",
    "        for col in label_cols:\n",
    "            f.write(f\"\\nMetrics for {col}:\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            for metric, value in metrics_results[col].items():\n",
    "                if metric != 'confusion_matrix':\n",
    "                    f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "            f.write(\"\\nConfusion Matrix:\\n\")\n",
    "            cm = metrics_results[col]['confusion_matrix']\n",
    "            f.write(f\"TN: {cm['tn']}, FP: {cm['fp']}\\n\")\n",
    "            f.write(f\"FN: {cm['fn']}, TP: {cm['tp']}\\n\")\n",
    "        \n",
    "        # Write mean metrics\n",
    "        f.write(\"\\n\\nMEAN METRICS ACROSS ALL LABELS\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        for metric, value in metrics_results['mean_metrics'].items():\n",
    "            f.write(f\"Mean {metric}: {value:.4f}\\n\")\n",
    "\n",
    "def calculate_mean_column_wise_auc(predictions_df, labels_df):\n",
    "    \"\"\"Calculate AUC scores for each label\"\"\"\n",
    "    print(\"\\nCalculating AUC scores...\")\n",
    "    auc_scores = {}\n",
    "    \n",
    "    for column in label_cols:\n",
    "        y_pred = predictions_df[column].values\n",
    "        y_true = labels_df[column].values\n",
    "        \n",
    "        # Filter out -1 labels\n",
    "        valid_indices = y_true != -1\n",
    "        y_pred_filtered = y_pred[valid_indices]\n",
    "        y_true_filtered = y_true[valid_indices]\n",
    "        \n",
    "        if len(np.unique(y_true_filtered)) > 1:\n",
    "            auc = roc_auc_score(y_true_filtered, y_pred_filtered)\n",
    "            auc_scores[column] = auc\n",
    "            print(f\"{column}: {auc:.4f}\")\n",
    "    \n",
    "    mean_auc = np.mean(list(auc_scores.values()))\n",
    "    print(f\"\\nMean ROC AUC Score: {mean_auc:.4f}\")\n",
    "    \n",
    "    # Visualize AUC scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(auc_scores.keys(), auc_scores.values())\n",
    "    plt.axhline(y=mean_auc, color='r', linestyle='--', \n",
    "                label=f'Mean AUC: {mean_auc:.4f}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('AUC Score')\n",
    "    plt.title('ROC AUC Scores by Label')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('auc_scores.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return mean_auc, auc_scores\n",
    "\n",
    "def pr(y_i, y, x):\n",
    "    \"\"\"Calculate prediction ratio with features matrix\"\"\"\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "def train_model(train_features, y):\n",
    "    \"\"\"Train model with Naive Bayes features\"\"\"\n",
    "    r = np.log(pr(1, y, train_features) / pr(0, y, train_features))\n",
    "    m = LogisticRegression(C=4)\n",
    "    x_nb = train_features.multiply(r)\n",
    "    return m.fit(x_nb, y), r\n",
    "\n",
    "def main():\n",
    "    # Load and prepare data\n",
    "    train, test, test_labels = load_and_prepare_data()\n",
    "    \n",
    "    # Analyze text examples\n",
    "    analyze_text_examples(train)\n",
    "    \n",
    "    # Analyze comment lengths\n",
    "    length_stats = analyze_comment_lengths(train)\n",
    "    \n",
    "    # Analyze label distribution\n",
    "    label_stats = analyze_label_distribution(train)\n",
    "\n",
    "    # Create general toxic/non-toxic word clouds\n",
    "    create_general_wordclouds(train, label_cols)\n",
    "    \n",
    "    # Create wordclouds for each label\n",
    "    for label in label_cols:\n",
    "        create_wordcloud_by_label(train, label, f'WordCloud for {label} Comments')\n",
    "    \n",
    "    # Analyze label correlations\n",
    "    correlation_matrix = analyze_correlations(train)\n",
    "    \n",
    "    # Extract features\n",
    "    train_features, test_features, vectorizer = extract_features(train, test)\n",
    "    \n",
    "    # Train models and make predictions\n",
    "    print(\"\\nTraining models and making predictions...\")\n",
    "    predictions = np.zeros((len(test), len(label_cols)))\n",
    "    \n",
    "    for i, col in enumerate(label_cols):\n",
    "        print(f'Training model for {col}...')\n",
    "        y = train[col].values\n",
    "        model, r = train_model(train_features, y)\n",
    "        predictions[:,i] = model.predict_proba(test_features.multiply(r))[:,1]\n",
    "    \n",
    "    # Create submission\n",
    "    submission = pd.DataFrame(predictions, columns=label_cols)\n",
    "    submission['id'] = test['id']\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "    # Calculate and visualize AUC scores\n",
    "    mean_auc, column_scores = calculate_mean_column_wise_auc(submission, test_labels)\n",
    "\n",
    "    print(\"\\nCalculating comprehensive metrics...\")\n",
    "    metrics_results = calculate_metrics(submission, test_labels)\n",
    "    \n",
    "    # Create visualizations for all metrics\n",
    "    visualize_all_metrics(metrics_results)\n",
    "    \n",
    "    # Save detailed metrics summary\n",
    "    save_metrics_summary(metrics_results)\n",
    "    \n",
    "    # Save all results\n",
    "    results = {\n",
    "        'length_statistics': length_stats,\n",
    "        'label_distribution': label_stats,\n",
    "        'auc_scores': column_scores,\n",
    "        'mean_auc': mean_auc,\n",
    "        'correlation_matrix': correlation_matrix.to_dict()\n",
    "    }\n",
    "    \n",
    "    with open('analysis_results.txt', 'w') as f:\n",
    "        f.write(\"ANALYSIS RESULTS\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        for key, value in results.items():\n",
    "            f.write(f\"{key.upper()}:\\n\")\n",
    "            f.write(str(value))\n",
    "            f.write(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Check the generated files for visualizations and detailed results.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpprj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
