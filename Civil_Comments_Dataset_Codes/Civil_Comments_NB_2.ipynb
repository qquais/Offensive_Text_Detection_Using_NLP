{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project: Detecting Offensive Text Using NLP Techniques\n",
    "# Subject: CS 59000-05 Natural Language Processing\n",
    "# Author: \n",
    "# Qurratul Ain Quais : quaiqa@pfw.edu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required installations:\n",
    "# pip install pandas scikit-learn matplotlib seaborn wordcloud spacy\n",
    "# python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from wordcloud import WordCloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/nlpprj/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models and making predictions...\n",
      "Training model for toxicity...\n",
      "Training model for obscene...\n",
      "Training model for threat...\n",
      "Training model for insult...\n",
      "Training model for identity_attack...\n",
      "Training model for sexual_explicit...\n",
      "\n",
      "Calculating comprehensive metrics...\n",
      "\n",
      "Metrics for toxicity:\n",
      "Precision: 0.7676\n",
      "Recall: 0.5267\n",
      "F1-score: 0.6247\n",
      "Accuracy: 0.9494\n",
      "AUC: 0.9533\n",
      "\n",
      "Metrics for obscene:\n",
      "Precision: 0.6962\n",
      "Recall: 0.4346\n",
      "F1-score: 0.5351\n",
      "Accuracy: 0.9958\n",
      "AUC: 0.9836\n",
      "\n",
      "Metrics for threat:\n",
      "Precision: 0.3500\n",
      "Recall: 0.1267\n",
      "F1-score: 0.1860\n",
      "Accuracy: 0.9975\n",
      "AUC: 0.9651\n",
      "\n",
      "Metrics for insult:\n",
      "Precision: 0.7657\n",
      "Recall: 0.5608\n",
      "F1-score: 0.6474\n",
      "Accuracy: 0.9637\n",
      "AUC: 0.9663\n",
      "\n",
      "Metrics for identity_attack:\n",
      "Precision: 0.4430\n",
      "Recall: 0.2038\n",
      "F1-score: 0.2792\n",
      "Accuracy: 0.9926\n",
      "AUC: 0.9745\n",
      "\n",
      "Metrics for sexual_explicit:\n",
      "Precision: 0.6000\n",
      "Recall: 0.2625\n",
      "F1-score: 0.3652\n",
      "Accuracy: 0.9977\n",
      "AUC: 0.9882\n",
      "\n",
      "Analyzing label distribution...\n",
      "\n",
      "Creating word clouds for specific toxicity types...\n",
      "Creating wordcloud for toxicity...\n",
      "Creating wordcloud for obscene...\n",
      "Creating wordcloud for threat...\n",
      "Creating wordcloud for insult...\n",
      "Creating wordcloud for identity_attack...\n",
      "Creating wordcloud for sexual_explicit...\n",
      "\n",
      "Analyzing label correlations...\n",
      "\n",
      "Analysis complete! Check the generated files for visualizations and detailed results.\n"
     ]
    }
   ],
   "source": [
    "# Define labels globally\n",
    "label_cols = ['toxicity', 'obscene', 'threat', \n",
    "              'insult', 'identity_attack', 'sexual_explicit']\n",
    "\n",
    "# Load dataset\n",
    "def load_and_prepare_data():\n",
    "    print(\"Loading dataset...\")\n",
    "    ds = load_dataset(\"google/civil_comments\")\n",
    "    \n",
    "    # Convert to pandas and prepare\n",
    "    train = ds[\"train\"].to_pandas()\n",
    "    test = ds[\"test\"].to_pandas()\n",
    "    \n",
    "    # Add unique ID to test data\n",
    "    test = test.reset_index(drop=True)\n",
    "    test['id'] = test.index\n",
    "    \n",
    "    # Handle empty comments\n",
    "    train['text'] = train['text'].fillna(\"unknown\")\n",
    "    test['text'] = test['text'].fillna(\"unknown\")\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# Tokenization\n",
    "re_tok = re.compile(f'([{string.punctuation}\"\"¨«»®´·º½¾¿¡§£₤''])')\n",
    "def tokenize(s): \n",
    "    return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "# Feature extraction\n",
    "def extract_features(train_df, test_df, text_column='text'):\n",
    "    print(\"Extracting features...\")\n",
    "    vec = TfidfVectorizer(\n",
    "        ngram_range=(1,2),\n",
    "        tokenizer=tokenize,\n",
    "        min_df=3,\n",
    "        max_df=0.9,\n",
    "        strip_accents='unicode',\n",
    "        use_idf=True,\n",
    "        smooth_idf=True,\n",
    "        sublinear_tf=True\n",
    "    )\n",
    "    \n",
    "    train_features = vec.fit_transform(train_df[text_column])\n",
    "    test_features = vec.transform(test_df[text_column])\n",
    "    \n",
    "    return train_features, test_features, vec\n",
    "\n",
    "# Model training utilities\n",
    "def pr(y_i, y, x):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "def train_model(x, y):\n",
    "    \"\"\"Train logistic regression model with Naive Bayes features\"\"\"\n",
    "    y_binary = (y.values >= 0.5).astype(int)\n",
    "    r = np.log(pr(1, y_binary, x) / pr(0, y_binary, x))\n",
    "    m = LogisticRegression(C=4, max_iter=200)\n",
    "    x_nb = x.multiply(r)\n",
    "    return m.fit(x_nb, y_binary), r\n",
    "\n",
    "def calculate_metrics(predictions_df, test_df):\n",
    "    \"\"\"Calculate comprehensive metrics including Precision, Recall, F1-score, and Accuracy\"\"\"\n",
    "    metrics_results = {}\n",
    "    \n",
    "    # Ensure predictions and test data are aligned by id\n",
    "    predictions_df = predictions_df.set_index('id')\n",
    "    test_df = test_df.set_index('id')\n",
    "    \n",
    "    # Verify that IDs match\n",
    "    if not predictions_df.index.equals(test_df.index):\n",
    "        raise ValueError(\"Prediction and test set IDs don't match!\")\n",
    "    \n",
    "    for col in label_cols:\n",
    "        # Convert predictions to binary using 0.5 threshold\n",
    "        y_pred_binary = (predictions_df[col] >= 0.5).astype(int)\n",
    "        y_true = (test_df[col] >= 0.5).astype(int)\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred_binary).ravel()\n",
    "        \n",
    "        # Calculate metrics manually to avoid classification_report issues\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        \n",
    "        metrics_results[col] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'accuracy': accuracy,\n",
    "            'auc': roc_auc_score(y_true, predictions_df[col]) if len(np.unique(y_true)) > 1 else 0,\n",
    "            'confusion_matrix': {\n",
    "                'tn': tn,\n",
    "                'fp': fp,\n",
    "                'fn': fn,\n",
    "                'tp': tp\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Print metrics for current label\n",
    "        print(f\"\\nMetrics for {col}:\")\n",
    "        print(f\"Precision: {metrics_results[col]['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics_results[col]['recall']:.4f}\")\n",
    "        print(f\"F1-score: {metrics_results[col]['f1_score']:.4f}\")\n",
    "        print(f\"Accuracy: {metrics_results[col]['accuracy']:.4f}\")\n",
    "        print(f\"AUC: {metrics_results[col]['auc']:.4f}\")\n",
    "        \n",
    "        # Create confusion matrix visualization\n",
    "        cm = np.array([[tn, fp], [fn, tp]])\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix for {col}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.savefig(f'confusion_matrix_{col}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    return metrics_results\n",
    "\n",
    "def visualize_all_metrics(metrics_results):\n",
    "    \"\"\"Create a comprehensive visualization of all metrics\"\"\"\n",
    "    metrics_to_plot = ['precision', 'recall', 'f1_score', 'accuracy', 'auc']\n",
    "    n_metrics = len(metrics_to_plot)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    width = 0.15\n",
    "    x = np.arange(len(label_cols))\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        values = [metrics_results[col][metric] for col in label_cols]\n",
    "        plt.bar(x + (i - n_metrics/2 + 0.5) * width, values, width, label=metric.replace('_', ' ').title())\n",
    "    \n",
    "    plt.xlabel('Labels')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('All Metrics Comparison Across Labels')\n",
    "    plt.xticks(x, label_cols, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('all_metrics_comparison.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def calculate_auc_scores(predictions_df, test_df):\n",
    "    \"\"\"Calculate AUC scores using aligned IDs.\"\"\"\n",
    "    # Ensure predictions and test data are aligned by id\n",
    "    predictions_df = predictions_df.set_index('id')\n",
    "    test_df = test_df.set_index('id')\n",
    "    \n",
    "    # Verify that IDs match\n",
    "    if not predictions_df.index.equals(test_df.index):\n",
    "        raise ValueError(\"Prediction and test set IDs don't match!\")\n",
    "    \n",
    "    auc_scores = {}\n",
    "    \n",
    "    for col in label_cols:\n",
    "        # Get predictions and true labels, ensuring they're aligned by ID\n",
    "        y_pred = predictions_df[col]\n",
    "        y_true = (test_df[col] >= 0.5).astype(int)\n",
    "        \n",
    "        if len(np.unique(y_true)) > 1:\n",
    "            auc = roc_auc_score(y_true, y_pred)\n",
    "            auc_scores[col] = auc\n",
    "            print(f\"{col}: {auc:.4f}\")\n",
    "    \n",
    "    mean_auc = np.mean(list(auc_scores.values()))\n",
    "    print(f\"\\nMean ROC AUC Score: {mean_auc:.4f}\")\n",
    "    \n",
    "    return mean_auc, auc_scores\n",
    "\n",
    "def visualize_auc_scores(column_scores, mean_auc):\n",
    "    \"\"\"Visualize AUC scores for each label\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    valid_labels = list(column_scores.keys())\n",
    "    scores = [column_scores[label] for label in valid_labels]\n",
    "\n",
    "    plt.bar(valid_labels, scores)\n",
    "    plt.axhline(y=mean_auc, color='r', linestyle='--', label=f'Mean AUC: {mean_auc:.4f}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('AUC Score')\n",
    "    plt.title('ROC AUC Scores by Label')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('auc_scores.png')\n",
    "    plt.close()\n",
    "\n",
    "def analyze_label_distribution(train_df, test_df):\n",
    "    \"\"\"Analyze and visualize label distribution\"\"\"\n",
    "    print(\"\\nAnalyzing label distribution...\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    train_stats = {col: (train_df[col] >= 0.5).mean() * 100 for col in label_cols}\n",
    "    test_stats = {col: (test_df[col] >= 0.5).mean() * 100 for col in label_cols}\n",
    "    \n",
    "    x = np.arange(len(label_cols))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, train_stats.values(), width, label='Train')\n",
    "    plt.bar(x + width/2, test_stats.values(), width, label='Test')\n",
    "    \n",
    "    plt.xlabel('Labels')\n",
    "    plt.ylabel('Percentage of Positive Cases')\n",
    "    plt.title('Distribution of Labels in Train and Test Sets')\n",
    "    plt.xticks(x, label_cols, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('label_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return train_stats, test_stats\n",
    "\n",
    "def create_wordcloud(text_series, title):\n",
    "    \"\"\"Create and save wordcloud visualization\"\"\"\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(text_series))\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(f'{title.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.close()\n",
    "\n",
    "def create_wordcloud_by_label(df, label, title):\n",
    "    \"\"\"Create wordcloud for specific label\"\"\"\n",
    "    print(f\"Creating wordcloud for {label}...\")\n",
    "    \n",
    "    # Filter comments where the label value is >= 0.5 (toxic)\n",
    "    toxic_comments = df[df[label] >= 0.5]['text']\n",
    "    \n",
    "    if len(toxic_comments) == 0:\n",
    "        print(f\"No comments found for {label}\")\n",
    "        return\n",
    "        \n",
    "    text = ' '.join(toxic_comments)\n",
    "    wordcloud = WordCloud(width=800, height=400, \n",
    "                         background_color='white',\n",
    "                         max_words=200).generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(f'wordcloud_{label}.png')\n",
    "    plt.close()\n",
    "\n",
    "def analyze_correlations(predictions_df):\n",
    "    \"\"\"Analyze and visualize correlations between predictions\"\"\"\n",
    "    print(\"\\nAnalyzing label correlations...\")\n",
    "    corr_matrix = predictions_df[label_cols].corr()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Label Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return corr_matrix\n",
    "\n",
    "def main():\n",
    "    # Load and prepare data\n",
    "    train, test = load_and_prepare_data()\n",
    "    \n",
    "    # Extract features\n",
    "    train_features, test_features, vectorizer = extract_features(train, test)\n",
    "    \n",
    "    # Train models and make predictions\n",
    "    print(\"Training models and making predictions...\")\n",
    "    predictions = np.zeros((len(test), len(label_cols)))\n",
    "    for i, col in enumerate(label_cols):\n",
    "        print(f'Training model for {col}...')\n",
    "        model, r = train_model(train_features, train[col])\n",
    "        predictions[:,i] = model.predict_proba(test_features.multiply(r))[:,1]\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    predictions_df = pd.DataFrame(predictions, columns=label_cols)\n",
    "    predictions_df['id'] = test['id']\n",
    "    predictions_df.to_csv('submission.csv', index=False)\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    print(\"\\nCalculating comprehensive metrics...\")\n",
    "    metrics_results = calculate_metrics(predictions_df, test)\n",
    "    \n",
    "    # Visualize all metrics\n",
    "    visualize_all_metrics(metrics_results)\n",
    "    \n",
    "    # Previous visualizations and analysis\n",
    "    train_stats, test_stats = analyze_label_distribution(train, test)\n",
    "    \n",
    "    # Create word clouds\n",
    "    toxic_mask = predictions_df['toxicity'] >= 0.5\n",
    "    create_wordcloud(test[toxic_mask]['text'], 'Toxic Comments WordCloud')\n",
    "    create_wordcloud(test[~toxic_mask]['text'], 'Non-Toxic Comments WordCloud')\n",
    "\n",
    "    print(\"\\nCreating word clouds for specific toxicity types...\")\n",
    "    for label in label_cols:\n",
    "        create_wordcloud_by_label(train, label, f'WordCloud for {label} Comments')\n",
    "    \n",
    "    # Analyze correlations\n",
    "    correlation_matrix = analyze_correlations(predictions_df)\n",
    "    \n",
    "    # Save detailed results including new metrics\n",
    "    results = {\n",
    "        'metrics_by_label': metrics_results,\n",
    "        'train_distribution': train_stats,\n",
    "        'test_distribution': test_stats,\n",
    "        'correlation_matrix': correlation_matrix.to_dict()\n",
    "    }\n",
    "    \n",
    "    # Save results to file\n",
    "    with open('analysis_results.txt', 'w') as f:\n",
    "        f.write(\"ANALYSIS RESULTS\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        for key, value in results.items():\n",
    "            f.write(f\"{key.upper()}:\\n\")\n",
    "            f.write(str(value))\n",
    "            f.write(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Check the generated files for visualizations and detailed results.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpprj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
