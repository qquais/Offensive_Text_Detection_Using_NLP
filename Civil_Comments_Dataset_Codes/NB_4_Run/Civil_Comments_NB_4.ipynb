{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project: Detecting Offensive Text Using NLP Techniques\n",
    "# Subject: CS 59000-05 Natural Language Processing\n",
    "# Author: \n",
    "# Qurratul Ain Quais : quaiqa@pfw.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required installations:\n",
    "# pip install pandas scikit-learn matplotlib seaborn wordcloud spacy\n",
    "# python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from wordcloud import WordCloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/nlpprj/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models and making predictions...\n",
      "Training model for toxicity...\n",
      "Training model for obscene...\n",
      "Training model for threat...\n",
      "Training model for insult...\n",
      "Training model for identity_attack...\n",
      "Training model for sexual_explicit...\n",
      "\n",
      "Calculating comprehensive metrics...\n",
      "\n",
      "Metrics for toxicity:\n",
      "Precision: 0.7676\n",
      "Recall: 0.5267\n",
      "F1-score: 0.6247\n",
      "Accuracy: 0.9494\n",
      "AUC: 0.9533\n",
      "\n",
      "Metrics for obscene:\n",
      "Precision: 0.6962\n",
      "Recall: 0.4346\n",
      "F1-score: 0.5351\n",
      "Accuracy: 0.9958\n",
      "AUC: 0.9836\n",
      "\n",
      "Metrics for threat:\n",
      "Precision: 0.3500\n",
      "Recall: 0.1267\n",
      "F1-score: 0.1860\n",
      "Accuracy: 0.9975\n",
      "AUC: 0.9651\n",
      "\n",
      "Metrics for insult:\n",
      "Precision: 0.7657\n",
      "Recall: 0.5608\n",
      "F1-score: 0.6474\n",
      "Accuracy: 0.9637\n",
      "AUC: 0.9663\n",
      "\n",
      "Metrics for identity_attack:\n",
      "Precision: 0.4430\n",
      "Recall: 0.2038\n",
      "F1-score: 0.2792\n",
      "Accuracy: 0.9926\n",
      "AUC: 0.9745\n",
      "\n",
      "Metrics for sexual_explicit:\n",
      "Precision: 0.6000\n",
      "Recall: 0.2625\n",
      "F1-score: 0.3652\n",
      "Accuracy: 0.9977\n",
      "AUC: 0.9882\n",
      "\n",
      "Mean Metrics Across All Labels:\n",
      "Mean precision: 0.6038\n",
      "Mean recall: 0.3525\n",
      "Mean f1_score: 0.4396\n",
      "Mean accuracy: 0.9828\n",
      "Mean auc: 0.9718\n",
      "\n",
      "Analyzing label distribution...\n",
      "\n",
      "Creating word clouds for specific toxicity types...\n",
      "Creating wordcloud for toxicity...\n",
      "Creating wordcloud for obscene...\n",
      "Creating wordcloud for threat...\n",
      "Creating wordcloud for insult...\n",
      "Creating wordcloud for identity_attack...\n",
      "Creating wordcloud for sexual_explicit...\n",
      "\n",
      "Analyzing label correlations...\n",
      "\n",
      "Calculating advanced metrics...\n",
      "\n",
      "Advanced Metrics for toxicity:\n",
      "PR-AUC: 0.7279\n",
      "Average Precision: 0.7280\n",
      "MCC: 0.6311\n",
      "Optimal F1: 0.6612\n",
      "Optimal Threshold: 0.2393\n",
      "\n",
      "Advanced Metrics for obscene:\n",
      "PR-AUC: 0.6039\n",
      "Average Precision: 0.6046\n",
      "MCC: 0.5946\n",
      "Optimal F1: 0.5969\n",
      "Optimal Threshold: 0.1756\n",
      "\n",
      "Advanced Metrics for threat:\n",
      "PR-AUC: 0.1908\n",
      "Average Precision: 0.1933\n",
      "MCC: 0.3117\n",
      "Optimal F1: 0.3098\n",
      "Optimal Threshold: 0.0817\n",
      "\n",
      "Advanced Metrics for insult:\n",
      "PR-AUC: 0.7434\n",
      "Average Precision: 0.7434\n",
      "MCC: 0.6569\n",
      "Optimal F1: 0.6771\n",
      "Optimal Threshold: 0.2704\n",
      "\n",
      "Advanced Metrics for identity_attack:\n",
      "PR-AUC: 0.3159\n",
      "Average Precision: 0.3167\n",
      "MCC: 0.3933\n",
      "Optimal F1: 0.3947\n",
      "Optimal Threshold: 0.1218\n",
      "\n",
      "Advanced Metrics for sexual_explicit:\n",
      "PR-AUC: 0.4317\n",
      "Average Precision: 0.4363\n",
      "MCC: 0.5105\n",
      "Optimal F1: 0.5107\n",
      "Optimal Threshold: 0.1104\n",
      "\n",
      "Mean Advanced Metrics:\n",
      "Mean pr_auc: 0.5023\n",
      "Mean average_precision: 0.5037\n",
      "Mean mcc: 0.5163\n",
      "Mean f1_optimal: 0.5251\n",
      "Mean threshold_optimal: 0.1666\n",
      "\n",
      "Analysis complete! Check the generated files for visualizations and detailed results.\n"
     ]
    }
   ],
   "source": [
    "# Define labels globally\n",
    "label_cols = ['toxicity', 'obscene', 'threat', \n",
    "              'insult', 'identity_attack', 'sexual_explicit']\n",
    "\n",
    "# Load dataset\n",
    "def load_and_prepare_data():\n",
    "    print(\"Loading dataset...\")\n",
    "    ds = load_dataset(\"google/civil_comments\")\n",
    "    \n",
    "    # Convert to pandas and prepare\n",
    "    train = ds[\"train\"].to_pandas()\n",
    "    test = ds[\"test\"].to_pandas()\n",
    "    \n",
    "    # Add unique ID to test data\n",
    "    test = test.reset_index(drop=True)\n",
    "    test['id'] = test.index\n",
    "    \n",
    "    # Handle empty comments\n",
    "    train['text'] = train['text'].fillna(\"unknown\")\n",
    "    test['text'] = test['text'].fillna(\"unknown\")\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# Tokenization\n",
    "re_tok = re.compile(f'([{string.punctuation}\"\"¨«»®´·º½¾¿¡§£₤''])')\n",
    "def tokenize(s): \n",
    "    return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "# Feature extraction\n",
    "def extract_features(train_df, test_df, text_column='text'):\n",
    "    print(\"Extracting features...\")\n",
    "    vec = TfidfVectorizer(\n",
    "        ngram_range=(1,2),\n",
    "        tokenizer=tokenize,\n",
    "        min_df=3,\n",
    "        max_df=0.9,\n",
    "        strip_accents='unicode',\n",
    "        use_idf=True,\n",
    "        smooth_idf=True,\n",
    "        sublinear_tf=True\n",
    "    )\n",
    "    \n",
    "    train_features = vec.fit_transform(train_df[text_column])\n",
    "    test_features = vec.transform(test_df[text_column])\n",
    "    \n",
    "    return train_features, test_features, vec\n",
    "\n",
    "# Model training utilities\n",
    "def pr(y_i, y, x):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "def train_model(x, y):\n",
    "    \"\"\"Train logistic regression model with Naive Bayes features\"\"\"\n",
    "    y_binary = (y.values >= 0.5).astype(int)\n",
    "    r = np.log(pr(1, y_binary, x) / pr(0, y_binary, x))\n",
    "    m = LogisticRegression(C=4, max_iter=200)\n",
    "    x_nb = x.multiply(r)\n",
    "    return m.fit(x_nb, y_binary), r\n",
    "\n",
    "def calculate_metrics(predictions_df, test_df):\n",
    "    \"\"\"Calculate comprehensive metrics including Precision, Recall, F1-score, and Accuracy\"\"\"\n",
    "    metrics_results = {}\n",
    "    \n",
    "    # Ensure predictions and test data are aligned by id\n",
    "    predictions_df = predictions_df.set_index('id')\n",
    "    test_df = test_df.set_index('id')\n",
    "    \n",
    "    # Verify that IDs match\n",
    "    if not predictions_df.index.equals(test_df.index):\n",
    "        raise ValueError(\"Prediction and test set IDs don't match!\")\n",
    "    \n",
    "    # Lists to store metrics for mean calculation\n",
    "    all_metrics = {\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1_score': [],\n",
    "        'accuracy': [],\n",
    "        'auc': []\n",
    "    }\n",
    "    \n",
    "    for col in label_cols:\n",
    "        # Convert predictions to binary using 0.5 threshold\n",
    "        y_pred_binary = (predictions_df[col] >= 0.5).astype(int)\n",
    "        y_true = (test_df[col] >= 0.5).astype(int)\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred_binary).ravel()\n",
    "        \n",
    "        # Calculate metrics manually to avoid classification_report issues\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        auc = roc_auc_score(y_true, predictions_df[col]) if len(np.unique(y_true)) > 1 else 0\n",
    "        \n",
    "        # Store metrics for mean calculation\n",
    "        all_metrics['precision'].append(precision)\n",
    "        all_metrics['recall'].append(recall)\n",
    "        all_metrics['f1_score'].append(f1)\n",
    "        all_metrics['accuracy'].append(accuracy)\n",
    "        all_metrics['auc'].append(auc)\n",
    "        \n",
    "        metrics_results[col] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'accuracy': accuracy,\n",
    "            'auc': auc,\n",
    "            'confusion_matrix': {\n",
    "                'tn': tn,\n",
    "                'fp': fp,\n",
    "                'fn': fn,\n",
    "                'tp': tp\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Print metrics for current label\n",
    "        print(f\"\\nMetrics for {col}:\")\n",
    "        print(f\"Precision: {metrics_results[col]['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics_results[col]['recall']:.4f}\")\n",
    "        print(f\"F1-score: {metrics_results[col]['f1_score']:.4f}\")\n",
    "        print(f\"Accuracy: {metrics_results[col]['accuracy']:.4f}\")\n",
    "        print(f\"AUC: {metrics_results[col]['auc']:.4f}\")\n",
    "        \n",
    "        # Create confusion matrix visualization\n",
    "        cm = np.array([[tn, fp], [fn, tp]])\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix for {col}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.savefig(f'confusion_matrix_{col}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Calculate mean metrics\n",
    "    mean_metrics = {metric: np.mean(values) for metric, values in all_metrics.items()}\n",
    "    \n",
    "    # Print mean metrics\n",
    "    print(\"\\nMean Metrics Across All Labels:\")\n",
    "    for metric, value in mean_metrics.items():\n",
    "        print(f\"Mean {metric}: {value:.4f}\")\n",
    "    \n",
    "    # Add mean metrics to results\n",
    "    metrics_results['mean_metrics'] = mean_metrics\n",
    "    \n",
    "    return metrics_results\n",
    "\n",
    "def calculate_advanced_metrics(predictions_df, test_df):\n",
    "    \"\"\"Calculate advanced metrics including PR-AUC, AP, and MCC\"\"\"\n",
    "    from sklearn.metrics import (average_precision_score, matthews_corrcoef, \n",
    "                               precision_recall_curve, auc)\n",
    "    \n",
    "    # Ensure predictions and test data are aligned by id\n",
    "    predictions_df = predictions_df.set_index('id')\n",
    "    test_df = test_df.set_index('id')\n",
    "    \n",
    "    # Verify that IDs match\n",
    "    if not predictions_df.index.equals(test_df.index):\n",
    "        raise ValueError(\"Prediction and test set IDs don't match!\")\n",
    "    \n",
    "    advanced_metrics = {}\n",
    "    all_metrics = {\n",
    "        'pr_auc': [],\n",
    "        'average_precision': [],\n",
    "        'mcc': [],\n",
    "        'f1_optimal': [],\n",
    "        'threshold_optimal': []\n",
    "    }\n",
    "    \n",
    "    for col in label_cols:\n",
    "        y_pred = predictions_df[col]\n",
    "        y_true = (test_df[col] >= 0.5).astype(int)\n",
    "        \n",
    "        # Calculate PR curve and PR-AUC\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        \n",
    "        # Calculate Average Precision\n",
    "        ap = average_precision_score(y_true, y_pred)\n",
    "        \n",
    "        # Find optimal threshold using F1 score\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "        optimal_idx = np.argmax(f1_scores)\n",
    "        optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5\n",
    "        optimal_f1 = f1_scores[optimal_idx]\n",
    "        \n",
    "        # Calculate MCC with optimal threshold\n",
    "        y_pred_binary = (y_pred >= optimal_threshold).astype(int)\n",
    "        mcc = matthews_corrcoef(y_true, y_pred_binary)\n",
    "        \n",
    "        advanced_metrics[col] = {\n",
    "            'pr_auc': pr_auc,\n",
    "            'average_precision': ap,\n",
    "            'mcc': mcc,\n",
    "            'f1_optimal': optimal_f1,\n",
    "            'threshold_optimal': optimal_threshold,\n",
    "            'pr_curve': {\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'thresholds': thresholds\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Store for averaging\n",
    "        all_metrics['pr_auc'].append(pr_auc)\n",
    "        all_metrics['average_precision'].append(ap)\n",
    "        all_metrics['mcc'].append(mcc)\n",
    "        all_metrics['f1_optimal'].append(optimal_f1)\n",
    "        all_metrics['threshold_optimal'].append(optimal_threshold)\n",
    "        \n",
    "        print(f\"\\nAdvanced Metrics for {col}:\")\n",
    "        print(f\"PR-AUC: {pr_auc:.4f}\")\n",
    "        print(f\"Average Precision: {ap:.4f}\")\n",
    "        print(f\"MCC: {mcc:.4f}\")\n",
    "        print(f\"Optimal F1: {optimal_f1:.4f}\")\n",
    "        print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "    \n",
    "    # Calculate mean metrics\n",
    "    advanced_metrics['mean'] = {\n",
    "        metric: np.mean(values) for metric, values in all_metrics.items()\n",
    "    }\n",
    "    \n",
    "    print(\"\\nMean Advanced Metrics:\")\n",
    "    for metric, value in advanced_metrics['mean'].items():\n",
    "        print(f\"Mean {metric}: {value:.4f}\")\n",
    "    \n",
    "    return advanced_metrics\n",
    "\n",
    "def plot_pr_curves(advanced_metrics):\n",
    "    \"\"\"Plot precision-recall curves for all labels\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, col in enumerate(label_cols, 1):\n",
    "        plt.subplot(2, 3, i)\n",
    "        precision = advanced_metrics[col]['pr_curve']['precision']\n",
    "        recall = advanced_metrics[col]['pr_curve']['recall']\n",
    "        pr_auc = advanced_metrics[col]['pr_auc']\n",
    "        ap = advanced_metrics[col]['average_precision']\n",
    "        \n",
    "        plt.plot(recall, precision, label=f'PR-AUC: {pr_auc:.4f}\\nAP: {ap:.4f}')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title(f'{col}')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pr_curves.png')\n",
    "    plt.close()\n",
    "\n",
    "def visualize_all_metrics(metrics_results):\n",
    "    \"\"\"Create a comprehensive visualization of all metrics\"\"\"\n",
    "    metrics_to_plot = ['precision', 'recall', 'f1_score', 'accuracy', 'auc']\n",
    "    n_metrics = len(metrics_to_plot)\n",
    "    \n",
    "    # Create main metrics comparison plot\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    width = 0.15\n",
    "    x = np.arange(len(label_cols))\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        values = [metrics_results[col][metric] for col in label_cols]\n",
    "        bars = plt.bar(x + (i - n_metrics/2 + 0.5) * width, values, width, \n",
    "                      label=metric.replace('_', ' ').title())\n",
    "        \n",
    "        # Add mean line for each metric\n",
    "        mean_value = metrics_results['mean_metrics'][metric]\n",
    "        plt.axhline(y=mean_value, color=bars.patches[0].get_facecolor(), \n",
    "                   linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.xlabel('Labels')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('All Metrics Comparison Across Labels (with Means)')\n",
    "    plt.xticks(x, label_cols, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('all_metrics_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create mean metrics summary plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    mean_values = [metrics_results['mean_metrics'][metric] for metric in metrics_to_plot]\n",
    "    \n",
    "    bars = plt.bar(metrics_to_plot, mean_values)\n",
    "    plt.title('Mean Metrics Across All Labels')\n",
    "    plt.xlabel('Metric')\n",
    "    plt.ylabel('Mean Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mean_metrics_summary.png')\n",
    "    plt.close()\n",
    "\n",
    "def save_metrics_summary(metrics_results):\n",
    "    \"\"\"Save a detailed summary of all metrics to a file\"\"\"\n",
    "    with open('metrics_summary.txt', 'w') as f:\n",
    "        f.write(\"METRICS SUMMARY REPORT\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        # Write individual label metrics\n",
    "        for col in label_cols:\n",
    "            f.write(f\"\\nMetrics for {col}:\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            for metric, value in metrics_results[col].items():\n",
    "                if metric != 'confusion_matrix':\n",
    "                    f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "            f.write(\"\\nConfusion Matrix:\\n\")\n",
    "            cm = metrics_results[col]['confusion_matrix']\n",
    "            f.write(f\"TN: {cm['tn']}, FP: {cm['fp']}\\n\")\n",
    "            f.write(f\"FN: {cm['fn']}, TP: {cm['tp']}\\n\")\n",
    "        \n",
    "        # Write mean metrics\n",
    "        f.write(\"\\n\\nMEAN METRICS ACROSS ALL LABELS\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        for metric, value in metrics_results['mean_metrics'].items():\n",
    "            f.write(f\"Mean {metric}: {value:.4f}\\n\")\n",
    "\n",
    "\n",
    "def calculate_auc_scores(predictions_df, test_df):\n",
    "    \"\"\"Calculate AUC scores using aligned IDs.\"\"\"\n",
    "    # Ensure predictions and test data are aligned by id\n",
    "    predictions_df = predictions_df.set_index('id')\n",
    "    test_df = test_df.set_index('id')\n",
    "    \n",
    "    # Verify that IDs match\n",
    "    if not predictions_df.index.equals(test_df.index):\n",
    "        raise ValueError(\"Prediction and test set IDs don't match!\")\n",
    "    \n",
    "    auc_scores = {}\n",
    "    \n",
    "    for col in label_cols:\n",
    "        # Get predictions and true labels, ensuring they're aligned by ID\n",
    "        y_pred = predictions_df[col]\n",
    "        y_true = (test_df[col] >= 0.5).astype(int)\n",
    "        \n",
    "        if len(np.unique(y_true)) > 1:\n",
    "            auc = roc_auc_score(y_true, y_pred)\n",
    "            auc_scores[col] = auc\n",
    "            print(f\"{col}: {auc:.4f}\")\n",
    "    \n",
    "    mean_auc = np.mean(list(auc_scores.values()))\n",
    "    print(f\"\\nMean ROC AUC Score: {mean_auc:.4f}\")\n",
    "    \n",
    "    return mean_auc, auc_scores\n",
    "\n",
    "def visualize_auc_scores(column_scores, mean_auc):\n",
    "    \"\"\"Visualize AUC scores for each label\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    valid_labels = list(column_scores.keys())\n",
    "    scores = [column_scores[label] for label in valid_labels]\n",
    "\n",
    "    plt.bar(valid_labels, scores)\n",
    "    plt.axhline(y=mean_auc, color='r', linestyle='--', label=f'Mean AUC: {mean_auc:.4f}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('AUC Score')\n",
    "    plt.title('ROC AUC Scores by Label')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('auc_scores.png')\n",
    "    plt.close()\n",
    "\n",
    "def analyze_label_distribution(train_df, test_df):\n",
    "    \"\"\"Analyze and visualize label distribution\"\"\"\n",
    "    print(\"\\nAnalyzing label distribution...\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    train_stats = {col: (train_df[col] >= 0.5).mean() * 100 for col in label_cols}\n",
    "    test_stats = {col: (test_df[col] >= 0.5).mean() * 100 for col in label_cols}\n",
    "    \n",
    "    x = np.arange(len(label_cols))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, train_stats.values(), width, label='Train')\n",
    "    plt.bar(x + width/2, test_stats.values(), width, label='Test')\n",
    "    \n",
    "    plt.xlabel('Labels')\n",
    "    plt.ylabel('Percentage of Positive Cases')\n",
    "    plt.title('Distribution of Labels in Train and Test Sets')\n",
    "    plt.xticks(x, label_cols, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('label_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return train_stats, test_stats\n",
    "\n",
    "def create_wordcloud(text_series, title):\n",
    "    \"\"\"Create and save wordcloud visualization\"\"\"\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(text_series))\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(f'{title.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.close()\n",
    "\n",
    "def create_wordcloud_by_label(df, label, title):\n",
    "    \"\"\"Create wordcloud for specific label\"\"\"\n",
    "    print(f\"Creating wordcloud for {label}...\")\n",
    "    \n",
    "    # Filter comments where the label value is >= 0.5 (toxic)\n",
    "    toxic_comments = df[df[label] >= 0.5]['text']\n",
    "    \n",
    "    if len(toxic_comments) == 0:\n",
    "        print(f\"No comments found for {label}\")\n",
    "        return\n",
    "        \n",
    "    text = ' '.join(toxic_comments)\n",
    "    wordcloud = WordCloud(width=800, height=400, \n",
    "                         background_color='white',\n",
    "                         max_words=200).generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(f'wordcloud_{label}.png')\n",
    "    plt.close()\n",
    "\n",
    "def analyze_correlations(predictions_df):\n",
    "    \"\"\"Analyze and visualize correlations between predictions\"\"\"\n",
    "    print(\"\\nAnalyzing label correlations...\")\n",
    "    corr_matrix = predictions_df[label_cols].corr()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Label Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return corr_matrix\n",
    "\n",
    "def main():\n",
    "    # Load and prepare data\n",
    "    train, test = load_and_prepare_data()\n",
    "    \n",
    "    # Extract features\n",
    "    train_features, test_features, vectorizer = extract_features(train, test)\n",
    "    \n",
    "    # Train models and make predictions\n",
    "    print(\"Training models and making predictions...\")\n",
    "    predictions = np.zeros((len(test), len(label_cols)))\n",
    "    for i, col in enumerate(label_cols):\n",
    "        print(f'Training model for {col}...')\n",
    "        model, r = train_model(train_features, train[col])\n",
    "        predictions[:,i] = model.predict_proba(test_features.multiply(r))[:,1]\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    predictions_df = pd.DataFrame(predictions, columns=label_cols)\n",
    "    predictions_df['id'] = test['id']\n",
    "    predictions_df.to_csv('submission.csv', index=False)\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    print(\"\\nCalculating comprehensive metrics...\")\n",
    "    metrics_results = calculate_metrics(predictions_df, test)\n",
    "    \n",
    "    # Visualize all metrics\n",
    "    visualize_all_metrics(metrics_results)\n",
    "\n",
    "    # Save detailed metrics summary\n",
    "    save_metrics_summary(metrics_results)\n",
    "    \n",
    "    # Previous visualizations and analysis\n",
    "    train_stats, test_stats = analyze_label_distribution(train, test)\n",
    "    \n",
    "    # Create word clouds\n",
    "    toxic_mask = predictions_df['toxicity'] >= 0.5\n",
    "    create_wordcloud(test[toxic_mask]['text'], 'Toxic Comments WordCloud')\n",
    "    create_wordcloud(test[~toxic_mask]['text'], 'Non-Toxic Comments WordCloud')\n",
    "\n",
    "    print(\"\\nCreating word clouds for specific toxicity types...\")\n",
    "    for label in label_cols:\n",
    "        create_wordcloud_by_label(train, label, f'WordCloud for {label} Comments')\n",
    "    \n",
    "    # Analyze correlations\n",
    "    correlation_matrix = analyze_correlations(predictions_df)\n",
    "\n",
    "    # Calculate advanced metrics\n",
    "    print(\"\\nCalculating advanced metrics...\")\n",
    "    advanced_metrics = calculate_advanced_metrics(predictions_df, test)\n",
    "    \n",
    "    # Plot PR curves\n",
    "    plot_pr_curves(advanced_metrics)\n",
    "    \n",
    "    # Save detailed results including new metrics\n",
    "    results = {\n",
    "        'metrics_by_label': metrics_results,\n",
    "        'train_distribution': train_stats,\n",
    "        'test_distribution': test_stats,\n",
    "        'correlation_matrix': correlation_matrix.to_dict(),\n",
    "        'advanced_metrics': advanced_metrics,\n",
    "    }\n",
    "    \n",
    "    # Save results to file\n",
    "    with open('analysis_results.txt', 'w') as f:\n",
    "        f.write(\"ANALYSIS RESULTS\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        for key, value in results.items():\n",
    "            f.write(f\"{key.upper()}:\\n\")\n",
    "            f.write(str(value))\n",
    "            f.write(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Check the generated files for visualizations and detailed results.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpprj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
